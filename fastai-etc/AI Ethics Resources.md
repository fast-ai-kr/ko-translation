## AI 윤리 리소스 모음집
> 작성일: 2019년 8월 23일, 작성자: MaliciousJ, https://blog.naver.com/sposent7/221626305979

최근 제 'Ask-A-Data-Scientist' 글은 컴퓨터 공학을 공부하는 학생에게 ‘AI의 사회적 영향과 관련된 정책을 만드는 분야에서 어떻게 커리어를 추구하는지’에 대한 질문을 받고 영감을 받아 작성했습니다. 저는 아주 많은 좋은 정보들이 있고, 이것들을 한곳에 모으고 싶었습니다.


제 이전 글은 ['Ask-A-Data-Scientist advice columns here'](https://www.fast.ai/topics/#advice)에서 볼 수 있습니다.


기술에 몸담고 있는 사람들은 그들의 작업이 도덕적인 충돌을 일으키진 않을까 걱정하면서도, 적극적으로 이러한 걱정거리에 참여하고 있습니다.
[인류](https://qz.com/1016900/tracy-chou-leading-silicon-valley-engineer-explains-why-every-tech-worker-needs-a-humanities-education/)와 사회과학은 윤리적인 의문에 있어서 매우 밀접한 연관성과 중요성을 띠고 있습니다.
기술윤리가 아직 새로운 분야가 아닐 때(전통적으로 과학, 기술, 사회 및 정보과학 부서에서 연구 중일 때), 대부분의 기술 산업은 이러한 의문들에 눈을 뜨고 있으며, 이전보다 이 주제에 대한 관심이 훨씬 더 넓어지고 있습니다.


AI 윤리에 관해 일한다는 것은 많은 형태를 띠고 있습니다.
(기술 회사를 설립하거나, 윤리적인 방향으로 제품을 만들거나, 보다 정의로운 법률과 정책을 지지하고 일을 하거나, 질 나쁜 행위자들에게 책임을 지게 하려거나, 연구하고, 집필하고, 가르치는 등)
이 게시물의 나머지 부분에 추가적인 형태에 대한 많은 링크들과 몇 가지 구체적인 의견들을 포함시켰습니다.
이 목록에게 압도당하지 마시길!
이 게시물은 필요에 따라 참고할 수 있는 글이 되도록 작성했습니다.


- [전문적인 능력을 키워라](#build)
- [독서그룹을 시작하라](#start)(200가지 이상의 기술윤리 강의계획서에 대한 링크까지)
- [구독할만한 10가지 AI 윤리 전문가](#follow)
- [교육기관과 장학금](#fellowship)
- [자신만의 것을 만들어라](#create)
- [Fast.ai 관련 링크들](#links)


전반적인 AI의 윤리적 문제에 대한 개요를 살펴보고 싶다면, 저는 제 최근 'PyBay keynote'에 대한 발표를 확인해보라고 말하고 싶습니다.
부정적이거나 긍정적인 일련의 사례연구를 통해 저는 인간에게 해를 끼치는 기술에 대한 4가지 오해와 더불어 몇 가지 건강한 원칙을 제시합니다.


## 전문적인 능력을 키워라 <span id="build"></span>
AI의 사회적 영향에 대해 관심 있는 누군가를 위해, 저는 기계학습에 대한 지식을 키울 것을 추천합니다.
만약 당신이 프로그래머나 심층학습 관련 전문가로 일할 생각이 없더라도, 이는 어떻게 이러한 기술이 동작하는지, 어떻게 이 기술이 사용될 수 있는지 직접 이해하는데 유용할 것입니다.

저는 AI 윤리와 정책에 관심 있는 모두에게 파이썬을 배우는 것과 ['Practical Deep Learning for Coders course'](https://course.fast.ai/) 강의를 듣는 것을 권장합니다.
(유일한 선행조건은 1년 정도의 코딩 경험뿐입니다.)


## 독서그룹을 시작하라 <span id="start"></span>
University of Colorado Boulder의 정보과학 학부 교수 [Casey Fiesler](https://twitter.com/cfiesler)는 대중들을 소재로 한 [200개 이상의 기술윤리 강의](https://medium.com/@cfiesler/tech-ethics-curricula-a-collection-of-syllabi-3eedfb76be18)의 스프레드시트를 만들었고, 이 시트들의 대부분을 강의계획서와 관련지었습니다. 만약 여러분의 대학이 기술윤리 강의를 제공하지 않더라도, 나는 여러분이 기술윤리에 관한 모임, 독서그룹, 학생 주도 강의를 시작하기를 권장합니다. 그리고 이 강의계획서는 여러분의 것으로 만드는 데 도움이 될 것입니다.

대학교 학생이 아니더라도, 직장에서 기술윤리 독서그룹을 시작하는 것을 고려해보세요.
(아마 일주일에 한번 점심에 만나 서로 다른 독서를 하고 토론하는 것으로 충분합니다.)


## 구독할만한 10명의 AI 윤리 전문가 <span id="follow"></span>
제가 존경하고 구독하기를 추천하는 10명의 전문가를 소개합니다.
이들은 모두 대단한 논문, 회담 등을 몇 차례나 가지신 분들이며, 저는 단지 시작하는 당신을 위해 링크를 모아뒀을 뿐입니다.

- [Zeynep Tufekci](https://twitter.com/zeynep)는 UNC School의 정보 및 도서 과학의 교수로 [New York Times에 사설을 게재](https://www.nytimes.com/column/zeynep-tufekci)했습니다.
그녀의 MIT Tech Review article인 ['How social media took us from Tahrir Square to Donald Trump'](https://www.technologyreview.com/s/611806/how-social-media-took-us-from-tahrir-square-to-donald-trump/)를 읽어보세요.

- [Timnit Gebru](https://twitter.com/timnitGebru)는 Stanford에서 컴퓨터 과학 박사학위를 얻었습니다.
Microsoft Research에서 막 포스트닥터를 마친 그녀는 ['Black in AI'](https://blackinai.joleh.com/login)의 창립자입니다.
그녀의 논문 ['Datasheets for Datasets'](https://arxiv.org/abs/1803.09010)을 읽어보세요.

- [Latanya Sweeney](https://dataprivacylab.org/people/sweeney/)는 Harvard University에서 정부 및 기술부문의 교수이며,
[‘Technology Science’](https://techscience.org/)의 편집장과 Harvard Data Privacy Lab의 총괄을 맡고 있습니다.
그녀는 이전에 미국 연방 무역 위원회의 CTO(Chief Technical Officer)였습니다.
그녀의 FAT ML(Fairness, Accountability, and Transparency in Machine Learning) 발표인
['Saving Humanity'](https://www.youtube.com/watch?v=OlK_nVOM2tc)를 참고하세요.

- [Arvind Narayanan](https://twitter.com/random_walker)은 Princeton 컴퓨터 과학 교수로써
디지털 사생활과 정보보호, 암호화폐와 블록체인, AI 윤리와 기술정책에 대해 연구했습니다.
그의 FAT ML 튜토리얼 ['21 Definitions of Fairness'](https://www.youtube.com/watch?v=jIXIuYdnyyk)를 참고하세요.

- [Kate Crawford](https://twitter.com/katecrawford)는 뉴욕대학교 AINowInstitute의 공동 창립자로써 Microsoft에서 수석 연구원을 역임하며,
뉴욕대학원에서 주목받는 교수입니다. 그녀의 ['Politics of AI'](https://www.youtube.com/watch?v=HPopJb5aDyA) 대담을 시청해보세요.

- [danah boyd](https://twitter.com/zephoria)는 Microsoft Research의 수석 연구원이자
‘Data & Society’(사회과학 데이터를 연구하는 비영리 단체)의 창립자입니다.
그녀의 창립 연설 ['How an algorithmic world can be undermined'​](https://www.youtube.com/watch?v=NTl0yyPqf3E)을 시청해보세요.

- [Joy Buolamwini](https://twitter.com/jovialjoy)는 [‘Algorithmic Justice League’](https://www.ajlunited.org/)의 창립자이며 MIT Media Lab에서 박사학위를 막 마쳤습니다.
그녀의 [‘컴퓨터 비전에서 인종적 편향성에 대한 연구’](https://www.media.mit.edu/people/joyab/publications/)를 읽어보시길 바랍니다.

- [Renee DiResta](https://twitter.com/noUpside)는 New Knowledge(민주주의를 위한 비영리 데이터 정책의 수장격 회사) 연구 총괄을 맡고 있으며,
컴퓨터 선전과 정보 위반에 대해 의회에 증언한 적이 있습니다.
그녀는 Wired의 정기 기고자입니다. 그녀의 기고문 ['Up next: a better recommendation system'](https://www.wired.com/story/creating-ethical-recommendation-engines/)을 읽어보세요.

- [Alvaro Bedoya](https://twitter.com/alvarombedoya)는 Georgetown Law의 Privacy & Technology 센터에서 창립 총괄이사를 맡고 있습니다.
그는 개인 정보 문제(모바일 위치, 건강 데이터, NSA 투명성, 생체 인식)에 대한 상원 의원이었습니다.
그의 New York Times 기고문 ['Why Silicon Valley Lobbyists Love Big, Broad Privacy Bills'](https://www.nytimes.com/2018/04/11/opinion/silicon-valley-lobbyists-privacy.html)​을 읽어보세요

- 전(前) 유튜브 엔지니어인 [Guillaume Chaslot](https://twitter.com/gchaslot)는 [‘AlgoTransparency’](https://algotransparency.org/)의 창립자이며,
월스트리트저널, 가디언 지(紙)와 함께 [유튜브를 수사](https://medium.com/@guillaumechaslot/how-algorithms-can-learn-to-discredit-the-media-d1360157c4fa)했습니다.
그의 ['How Algorithms Can Learn to Discredit the Media'](https://www.wsj.com/articles/how-youtube-drives-viewers-to-the-internets-darkest-corners-1518020478)을 읽어보세요.

## 교육기관과 장학금 <span id="fellowship"></span>
아래 기관들은 이 분야에서 여러분의 자금을 절약하기 위해 모두 그들의 팟캐스트와 비디오 시청, 행사 참석, 인턴십, 장학금을 포함한 다양한 참여방법을 제공합니다. (여러분이 세상 어디에 있던 말이죠.)

- Harvard의 [Berkman Klein Center](https://cyber.harvard.edu/getinvolved)는 전 세계 사람들을 모아 인터넷이 선사하는 가장 큰 도전들에 대처하는 연구센터입니다. 그들의 프로그램은 기술자, 관리자, 정책 입안자들이 인공지능의 윤리 및 관리와 관련된 새로운 문제와 만날 수 있는 4달 간의  [장학금 프로그램](https://cyber.harvard.edu/getinvolved/fellowships) , [인턴십](https://cyber.harvard.edu/getinvolved/internships) , [집회](https://www.bkmla.org/) 를 포함하고 있습니다.

- [Data &Society](https://datasociety.net/engage/)는 비영리 연구소로써 뉴욕대학교의 danah boyd가 창립했습니다. 이들은 데이터 과학자와 엔지니어, 변호사와 사서, 민족 학자, 크리에이터, 역사가, 운동가들을 위해 1년간의 [장학금 프로그램](https://datasociety.net/engage/#fellows_program)을 열고 있습니다.

- [AI Now Institute](https://ainowinstitute.org/)는 Kate Crawford와 Meredith Whittaker에 의해 창립되었으며, NYU에 소재하고 있습니다. 그들은 ‘권리와 자유’, ‘노동과 자동화’, ‘편견과 포용’, ‘안전과 중요한 인프라’라는 4가지 영역에 초점을 맞추고 있습니다.

- [Georgetown Law Center on Privacy and Technology](https://www.law.georgetown.edu/privacy-technology-center/events/)는 사생활과 감시에 대한 법과 정책, 그리고 이들이 영향을 미치는 지역사회에 초점을 맞춘 싱크탱크입니다. 그들의 연구는 미국 경찰의 얼굴인식 기술의 규제되지 않은 사용에 대한 [끊임없는 연구](https://www.perpetuallineup.org/)를 포함하고 있습니다.

- [Data for Democracy](https://www.datafordemocracy.org/)는 ProPublica와의 몇몇 협력을 포함하여 광범위한 프로젝트에 참여해온 자원봉사자들의 비영리 단체입니다.

- [Mozilla Media Fellowships](https://foundation.mozilla.org/en/fellowships/)는 건강한 인터넷이 직면한 새로운 위협과 도전에 대처하는 방법에 대한 새로운 생각에 자금을 지원합니다. 관련된 프로젝트들은 양극화, 대량 감시, 가짜 뉴스 등을 다루기 위해 노력해왔습니다.

- [Knight Foundation](https://knightfoundation.org/programs/journalism)(저널리즘 분야 한정)은 디지털 시대에서 표현의 자유와 저널리즘의 우수성을 지원하기 위한 [AI 윤리 이니셔티브](https://aiethicsinitiative.org/challenge) 등의 프로그램에 자금을 지원합니다. 그들은 가짜 뉴스를 다루는 수많은 프로젝트를 지원해왔습니다.

- [Eyebeam Residency](https://www.eyebeam.org/what-we-do/)(예술가 한정)는 예술을 이용해 기술과 사회를 사로잡는 창의적인 일을 하는 이들을 위해 장학금을 제공하고 있습니다. 이전 프로젝트들은 오픈소스 교육 스타트 업인 littleBits (2009)과 첫 번째 페미니스트 위키피디아인 Edit-A-Thon (2013)을 포함합니다.

- [Aspen Tech Policy Hub Fellowship](https://www.aspentechpolicyhub.org/#/fellowship)은 기술 전문가를 위한 정책 과정을 가르치는 새 프로그램입니다. 이 과정 중에, 각 참여자는 모의 입법이나 정책 입안자, 백서용 툴킷, 혹은 애플리케이션 등 최소 1가지의 실용적인 정책 결과를 만들게 됩니다.


## 자신만의 것을 만들어라 <span id="create"></span>
만약 여러분이 원하는 것이 아직 세상에 없다면, 여러분은 자신만의 그룹이나, 단체, 비영리기구, 스타트업이 필요하다고 느낄 것입니다. 컴퓨터 비전 연구자 Timnit Gebru는 이 방면에서 아주 좋은 롤모델입니다. Gebru 박사는 2016년  [NIPS](https://nips.cc/)(AI 주요 학회)에 흑인 여성으로서 [자신의 경험을 묘사](https://www.technologyreview.com/s/610192/were-in-a-diversity-crisis-black-in-ais-founder-on-whats-poisoning-the-algorithms-in-our/)했습니다.

“제가 NIPS에 갔을 때 누군가 8500여 명의 사람이 왔더라고 하더군요. 저는 겨우 6명의 흑인밖에 셀 수 없었습니다. 저는 말 그대로 절망에 빠졌습니다. 제가 이 기분을 설명할 수 있는 유일한 방법이네요. 이 분야가 폭발적으로 성장해서, 주류 사회 곳곳에 영향을 미치고 있음을 보았는데도 말이에요.”

Gebru 박사는 흑인 AI 연구자들의 활발한 거대 네트워크, [‘Black in AI’](https://blackinai.joleh.com/login)를 만들기 위해 노력해왔으며, 이 네트워크는 회원들을 위한 새로운 연구 협력과 회의, 연설 초빙을 주도해왔고, 이는 구글의 첫 번째 AI 연구소를 가나의 수도 아크라에 세우기로 결정한 중요한 요인이 되었습니다.


## Fast.ai 관련 링크들 <span id="links"></span>
Fast.ai에서, 우리는 자주 윤리에 대해서 쓰고 말할 뿐만 아니라, 심층학습 과정에 주제를 포함시킵니다. 여러분이 흥미 있어 할 몇 가지 글을 가져왔습니다.

- [알고리즘과 편향성에 대해 HBR(Harvard Business Review)이 잘못 알고 있는 것](https://www.fast.ai/2018/08/07/hbr-bias-algorithms/)

- [데이터 과학이 민주주의를 파괴하고 대량학살을 용이하게 할 때](https://www.fast.ai/2017/11/02/ethics/)

- [페이스북과 윤리에 대해서 당신이 반드시 알아야 할 것](https://www.fast.ai/2018/04/19/facebook/)

- [AI에서 다양성의 위기(2017년 판)](https://www.fast.ai/2017/08/16/diversity-crisis/) 

  _역자 주 : 여기서 다양성이란 성별, 인종 등의 다양성을 의미한다_

- [AI에서 다양성의 위기, 그리고 Fast.ai의 다양성 장학금](https://www.fast.ai/2016/10/09/diversity-in-ai/)(2016년 판) 위와 같음

이 주제에 관해 그동안 우리가 나눴던 대화들이 있습니다.

- [기계학습에서 미처 깨닫지 못한 편향성의 분석 및 방지](https://www.infoq.com/presentations/unconscious-bias-machine-learning/) (QCon.ai 발표 자료)

- [워드임베딩(Word Embeddings), 기계학습의 편향성, 당신이 수학을 싫어하는 이유, 당신이 AI에게 필요한 이유](https://www.youtube.com/watch?v=25nC0n9ERq4&index=4&list=PLtmWHNX-gukJUPxzuBf_GnTfN67WJen6c)(Word2Vec과 같은 워드임베딩에서 일어나는 편향성에 관한 워크숍)

- [Fast.ai 강의 13 : 윤리 &이미지 향상](https://course.fast.ai/lessons/lesson13.html)

- [AI의 윤리와 편향성에 관한 몇 가지 건강한 원칙](https://www.youtube.com/watch?v=WC1kPtG8Iz8&index=6&list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6&t=0s) (PyBay 발표 자료)

기술의 윤리적인 영향은 거대하고 광범위한 영역이며, 해야 할 일은 여전히 많이 남아있습니다.
